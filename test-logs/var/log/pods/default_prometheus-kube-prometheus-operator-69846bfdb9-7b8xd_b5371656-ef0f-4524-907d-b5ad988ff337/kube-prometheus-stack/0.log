2023-12-20T20:20:57.475977749Z stdout F level=info ts=2023-12-20T20:20:57.475689819Z caller=main.go:180 msg="Starting Prometheus Operator" version="(version=0.70.0, branch=refs/tags/v0.70.0, revision=c2c673f7123f3745a2a982b4a2bdc43a11f50fad)"
2023-12-20T20:20:57.476041089Z stdout F level=info ts=2023-12-20T20:20:57.475827316Z caller=main.go:181 build_context="(go=go1.21.4, platform=linux/amd64, user=Action-Run-ID-7048794395, date=20231130-15:41:45, tags=unknown)"
2023-12-20T20:20:57.476080961Z stdout F level=info ts=2023-12-20T20:20:57.475876553Z caller=main.go:192 msg="namespaces filtering configuration " config="{allow_list=\"\",deny_list=\"\",prometheus_allow_list=\"\",alertmanager_allow_list=\"\",alertmanagerconfig_allow_list=\"\",thanosruler_allow_list=\"\"}"
2023-12-20T20:20:57.4873436Z stdout F level=info ts=2023-12-20T20:20:57.4871756Z caller=main.go:221 msg="connection established" cluster-version=v1.28.4
2023-12-20T20:20:57.505471572Z stdout F level=info ts=2023-12-20T20:20:57.50525954Z caller=operator.go:321 component=prometheusoperator msg="Kubernetes API capabilities" endpointslices=true
2023-12-20T20:20:57.520824398Z stdout F level=info ts=2023-12-20T20:20:57.520592937Z caller=operator.go:308 component=prometheusagentoperator msg="Kubernetes API capabilities" endpointslices=true
2023-12-20T20:20:57.533519047Z stdout F level=warn ts=2023-12-20T20:20:57.533275816Z caller=server.go:158 msg="server TLS client verification disabled" err="stat /etc/tls/private/tls-ca.crt: no such file or directory" client_ca_file=/etc/tls/private/tls-ca.crt
2023-12-20T20:20:57.543354165Z stdout F level=info ts=2023-12-20T20:20:57.542124047Z caller=server.go:259 msg="starting secure server" address=[::]:10250 http2=false
2023-12-20T20:20:57.634746459Z stdout F level=info ts=2023-12-20T20:20:57.634504389Z caller=operator.go:417 component=prometheusagentoperator msg="successfully synced all caches"
2023-12-20T20:20:57.63478462Z stdout F level=info ts=2023-12-20T20:20:57.634531151Z caller=operator.go:270 component=thanosoperator msg="successfully synced all caches"
2023-12-20T20:20:57.635259371Z stdout F level=info ts=2023-12-20T20:20:57.635047578Z caller=operator.go:309 component=alertmanageroperator msg="successfully synced all caches"
2023-12-20T20:20:57.635295823Z stdout F level=info ts=2023-12-20T20:20:57.63522676Z caller=operator.go:760 component=alertmanageroperator msg="StatefulSet not found" key=default/alertmanager-prometheus-kube-prometheus-alertmanager
2023-12-20T20:20:57.635421394Z stdout F level=info ts=2023-12-20T20:20:57.6352963Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2023-12-20T20:20:57.638087984Z stdout F level=info ts=2023-12-20T20:20:57.637915616Z caller=operator.go:378 component=prometheusoperator msg="successfully synced all caches"
2023-12-20T20:20:57.639522999Z stdout F level=info ts=2023-12-20T20:20:57.639318785Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T20:20:58.490789939Z stdout F level=info ts=2023-12-20T20:20:58.490573844Z caller=operator.go:760 component=alertmanageroperator msg="StatefulSet not found" key=default/alertmanager-prometheus-kube-prometheus-alertmanager
2023-12-20T20:20:58.589306754Z stdout F level=warn ts=2023-12-20T20:20:58.589108563Z caller=klog.go:96 component=k8s_client_runtime func=Warning msg="spec.template.spec.containers[1].ports[0]: duplicate port definition with spec.template.spec.initContainers[0].ports[0]"
2023-12-20T20:20:58.592912567Z stdout F level=info ts=2023-12-20T20:20:58.592613898Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2023-12-20T20:20:58.593002387Z stdout F level=info ts=2023-12-20T20:20:58.592793324Z caller=operator.go:760 component=alertmanageroperator msg="StatefulSet not found" key=default/alertmanager-prometheus-kube-prometheus-alertmanager
2023-12-20T20:20:59.48648177Z stdout F level=warn ts=2023-12-20T20:20:59.486254472Z caller=klog.go:96 component=k8s_client_runtime func=Warning msg="spec.template.spec.containers[1].ports[0]: duplicate port definition with spec.template.spec.initContainers[0].ports[0]"
2023-12-20T20:20:59.487879127Z stdout F level=info ts=2023-12-20T20:20:59.487711277Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T20:20:59.774187758Z stdout F level=info ts=2023-12-20T20:20:59.774011511Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2023-12-20T20:20:59.827270835Z stdout F level=info ts=2023-12-20T20:20:59.826992185Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2023-12-20T20:20:59.843734403Z stdout F level=info ts=2023-12-20T20:20:59.84354729Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T20:21:00.07283377Z stdout F level=info ts=2023-12-20T20:21:00.07259705Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T20:25:47.805909806Z stdout F level=info ts=2023-12-20T20:25:47.805711819Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T20:25:48.024647337Z stdout F level=info ts=2023-12-20T20:25:48.024402258Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2023-12-20T20:25:48.052984708Z stdout F level=info ts=2023-12-20T20:25:48.052710824Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T20:27:48.305702252Z stdout F level=info ts=2023-12-20T20:27:48.305469296Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T20:27:48.624348572Z stdout F level=info ts=2023-12-20T20:27:48.624071255Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2023-12-20T20:27:48.847768832Z stdout F level=info ts=2023-12-20T20:27:48.847569668Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T20:28:55.658769019Z stdout F level=info ts=2023-12-20T20:28:55.65858347Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T20:28:55.872920794Z stdout F level=info ts=2023-12-20T20:28:55.872603847Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2023-12-20T20:28:55.895565866Z stdout F level=info ts=2023-12-20T20:28:55.895382156Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T20:29:11.856477252Z stdout F level=info ts=2023-12-20T20:29:11.856209086Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T20:29:12.687189315Z stdout F level=info ts=2023-12-20T20:29:12.686895751Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2023-12-20T20:29:12.782258208Z stdout F level=info ts=2023-12-20T20:29:12.782087691Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T21:01:34.278769102Z stdout F level=info ts=2023-12-20T21:01:34.278490958Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T21:01:34.501873594Z stdout F level=info ts=2023-12-20T21:01:34.501547496Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2023-12-20T21:01:34.527191323Z stdout F level=info ts=2023-12-20T21:01:34.526925322Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T21:01:47.06007719Z stdout F level=info ts=2023-12-20T21:01:47.059810669Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T21:01:47.375068045Z stdout F level=info ts=2023-12-20T21:01:47.374710691Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2023-12-20T21:01:47.41884497Z stdout F level=info ts=2023-12-20T21:01:47.418644895Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T21:48:34.629226337Z stdout F level=info ts=2023-12-20T21:48:34.628932114Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T21:48:34.85201558Z stdout F level=info ts=2023-12-20T21:48:34.851814955Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2023-12-20T21:48:34.878634451Z stdout F level=info ts=2023-12-20T21:48:34.878468682Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T21:48:54.679706697Z stdout F level=info ts=2023-12-20T21:48:54.67944196Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2023-12-20T21:48:54.978235271Z stdout F level=info ts=2023-12-20T21:48:54.977899761Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2023-12-20T21:48:55.086472017Z stdout F level=info ts=2023-12-20T21:48:55.08626225Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2024-02-19T09:06:51.148443177Z stdout F level=info ts=2024-02-19T09:06:51.148093098Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2024-02-19T09:06:51.148477954Z stdout F level=info ts=2024-02-19T09:06:51.14812223Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2024-02-19T09:06:51.289079647Z stdout F level=info ts=2024-02-19T09:06:51.288857738Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2024-02-19T09:06:51.585532283Z stdout F level=info ts=2024-02-19T09:06:51.585283682Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2024-03-15T18:37:03.584183344Z stdout F level=warn ts=2024-03-15T18:37:03.583782294Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584243884Z stdout F level=warn ts=2024-03-15T18:37:03.583850122Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584254061Z stdout F level=warn ts=2024-03-15T18:37:03.58384017Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584283395Z stdout F level=warn ts=2024-03-15T18:37:03.583997904Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584290824Z stdout F level=warn ts=2024-03-15T18:37:03.584036642Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584324963Z stdout F level=warn ts=2024-03-15T18:37:03.584090808Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.ThanosRuler ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584333216Z stdout F level=warn ts=2024-03-15T18:37:03.584093517Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1alpha1.PrometheusAgent ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584343698Z stdout F level=warn ts=2024-03-15T18:37:03.584065874Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1alpha1.AlertmanagerConfig ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584350429Z stdout F level=warn ts=2024-03-15T18:37:03.584133228Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584363978Z stdout F level=warn ts=2024-03-15T18:37:03.584097692Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584370392Z stdout F level=warn ts=2024-03-15T18:37:03.584161736Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.Alertmanager ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584376606Z stdout F level=warn ts=2024-03-15T18:37:03.58415663Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.Probe ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584382973Z stdout F level=warn ts=2024-03-15T18:37:03.584192731Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.Prometheus ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584398808Z stdout F level=warn ts=2024-03-15T18:37:03.584180574Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.PodMonitor ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584405402Z stdout F level=warn ts=2024-03-15T18:37:03.584151137Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584426096Z stdout F level=warn ts=2024-03-15T18:37:03.584230434Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584433544Z stdout F level=warn ts=2024-03-15T18:37:03.584262553Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584440242Z stdout F level=warn ts=2024-03-15T18:37:03.584202691Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.PrometheusRule ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584447343Z stdout F level=warn ts=2024-03-15T18:37:03.584279246Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.58446204Z stdout F level=warn ts=2024-03-15T18:37:03.58416535Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.Namespace ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584469406Z stdout F level=warn ts=2024-03-15T18:37:03.584290878Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1alpha1.ScrapeConfig ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.58447578Z stdout F level=warn ts=2024-03-15T18:37:03.584004835Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.StatefulSet ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.58448201Z stdout F level=warn ts=2024-03-15T18:37:03.584213158Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.PodMonitor ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584497007Z stdout F level=warn ts=2024-03-15T18:37:03.584224064Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1alpha1.ScrapeConfig ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584505901Z stdout F level=warn ts=2024-03-15T18:37:03.584246113Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.Probe ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.584512272Z stdout F level=warn ts=2024-03-15T18:37:03.584285371Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.ServiceMonitor ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.58452593Z stdout F level=warn ts=2024-03-15T18:37:03.584246815Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.ServiceMonitor ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.58543401Z stdout F level=warn ts=2024-03-15T18:37:03.585265022Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.PartialObjectMetadata ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:03.585480193Z stdout F level=warn ts=2024-03-15T18:37:03.585166986Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: watch of *v1.PrometheusRule ended with: an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
2024-03-15T18:37:11.530216611Z stdout F level=warn ts=2024-03-15T18:37:11.529956849Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.Probe: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/probes?resourceVersion=12829624\": dial tcp 10.96.0.1:443: connect: network is unreachable"
2024-03-15T18:37:11.53037068Z stdout F level=error ts=2024-03-15T18:37:11.530251403Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.Probe: failed to list *v1.Probe: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/probes?resourceVersion=12829624\": dial tcp 10.96.0.1:443: connect: network is unreachable"
2024-03-15T18:37:16.262231961Z stdout F level=warn ts=2024-03-15T18:37:16.261991086Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.Probe: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/probes?resourceVersion=12829624\": dial tcp 10.96.0.1:443: connect: network is unreachable"
2024-03-15T18:37:16.262289181Z stdout F level=error ts=2024-03-15T18:37:16.262115217Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.Probe: failed to list *v1.Probe: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/probes?resourceVersion=12829624\": dial tcp 10.96.0.1:443: connect: network is unreachable"
2024-03-15T18:37:19.71804451Z stdout F level=warn ts=2024-03-15T18:37:19.717848768Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1alpha1.ScrapeConfig: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1alpha1/scrapeconfigs?resourceVersion=12829654\": dial tcp 10.96.0.1:443: connect: network is unreachable"
2024-03-15T18:37:19.718091207Z stdout F level=warn ts=2024-03-15T18:37:19.717863613Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get \"https://10.96.0.1:443/api/v1/namespaces?resourceVersion=12829602\": dial tcp 10.96.0.1:443: connect: network is unreachable"
2024-03-15T18:37:19.718122239Z stdout F level=error ts=2024-03-15T18:37:19.718061753Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1alpha1.ScrapeConfig: failed to list *v1alpha1.ScrapeConfig: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1alpha1/scrapeconfigs?resourceVersion=12829654\": dial tcp 10.96.0.1:443: connect: network is unreachable"
2024-03-15T18:37:19.718160283Z stdout F level=error ts=2024-03-15T18:37:19.718119411Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://10.96.0.1:443/api/v1/namespaces?resourceVersion=12829602\": dial tcp 10.96.0.1:443: connect: network is unreachable"
2024-03-15T18:37:34.539284872Z stdout F level=warn ts=2024-03-15T18:37:34.539026015Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get \"https://10.96.0.1:443/api/v1/configmaps?labelSelector=thanos-ruler-name&resourceVersion=12829691\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.53941717Z stdout F level=error ts=2024-03-15T18:37:34.539317649Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get \"https://10.96.0.1:443/api/v1/configmaps?labelSelector=thanos-ruler-name&resourceVersion=12829691\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.544556521Z stdout F level=warn ts=2024-03-15T18:37:34.544370908Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.ServiceMonitor: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/servicemonitors?resourceVersion=12829656\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.544582564Z stdout F level=error ts=2024-03-15T18:37:34.544472014Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.ServiceMonitor: failed to list *v1.ServiceMonitor: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/servicemonitors?resourceVersion=12829656\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.650014465Z stdout F level=warn ts=2024-03-15T18:37:34.649810741Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get \"https://10.96.0.1:443/api/v1/configmaps?labelSelector=prometheus-name&resourceVersion=12829603\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.650062562Z stdout F level=error ts=2024-03-15T18:37:34.649950233Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get \"https://10.96.0.1:443/api/v1/configmaps?labelSelector=prometheus-name&resourceVersion=12829603\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.661225541Z stdout F level=warn ts=2024-03-15T18:37:34.661065013Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.Prometheus: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/prometheuses?resourceVersion=12829604\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.661321632Z stdout F level=error ts=2024-03-15T18:37:34.661234111Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.Prometheus: failed to list *v1.Prometheus: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/prometheuses?resourceVersion=12829604\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.700554966Z stdout F level=warn ts=2024-03-15T18:37:34.700332971Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.PrometheusRule: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/prometheusrules?resourceVersion=12829719\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.700584229Z stdout F level=error ts=2024-03-15T18:37:34.70045789Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.PrometheusRule: failed to list *v1.PrometheusRule: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/prometheusrules?resourceVersion=12829719\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.713811171Z stdout F level=warn ts=2024-03-15T18:37:34.713608342Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get \"https://10.96.0.1:443/api/v1/namespaces?resourceVersion=12829624\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.713848014Z stdout F level=error ts=2024-03-15T18:37:34.713750669Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://10.96.0.1:443/api/v1/namespaces?resourceVersion=12829624\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.730007515Z stdout F level=warn ts=2024-03-15T18:37:34.729820707Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1alpha1.PrometheusAgent: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1alpha1/prometheusagents?resourceVersion=12829613\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.730040523Z stdout F level=error ts=2024-03-15T18:37:34.729934528Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1alpha1.PrometheusAgent: failed to list *v1alpha1.PrometheusAgent: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1alpha1/prometheusagents?resourceVersion=12829613\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.742146496Z stdout F level=warn ts=2024-03-15T18:37:34.741949097Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.StatefulSet: Get \"https://10.96.0.1:443/apis/apps/v1/statefulsets?resourceVersion=12829624\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.742175009Z stdout F level=error ts=2024-03-15T18:37:34.742047749Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://10.96.0.1:443/apis/apps/v1/statefulsets?resourceVersion=12829624\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.743375426Z stdout F level=warn ts=2024-03-15T18:37:34.743228507Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.Probe: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/probes?resourceVersion=12829601\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.743432127Z stdout F level=error ts=2024-03-15T18:37:34.743323287Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.Probe: failed to list *v1.Probe: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/probes?resourceVersion=12829601\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.772767343Z stdout F level=warn ts=2024-03-15T18:37:34.77256484Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.ThanosRuler: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/thanosrulers?resourceVersion=12829675\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.77279382Z stdout F level=error ts=2024-03-15T18:37:34.772676062Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.ThanosRuler: failed to list *v1.ThanosRuler: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/thanosrulers?resourceVersion=12829675\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.772902294Z stdout F level=warn ts=2024-03-15T18:37:34.772798946Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.StatefulSet: Get \"https://10.96.0.1:443/apis/apps/v1/statefulsets?resourceVersion=12829650\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.772991179Z stdout F level=error ts=2024-03-15T18:37:34.772929913Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://10.96.0.1:443/apis/apps/v1/statefulsets?resourceVersion=12829650\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.798187104Z stdout F level=warn ts=2024-03-15T18:37:34.797966086Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.StatefulSet: Get \"https://10.96.0.1:443/apis/apps/v1/statefulsets?resourceVersion=12829638\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.798213485Z stdout F level=error ts=2024-03-15T18:37:34.798117235Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://10.96.0.1:443/apis/apps/v1/statefulsets?resourceVersion=12829638\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.811354592Z stdout F level=warn ts=2024-03-15T18:37:34.811207406Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.PodMonitor: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/podmonitors?resourceVersion=12829704\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.811388199Z stdout F level=error ts=2024-03-15T18:37:34.811335728Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.PodMonitor: failed to list *v1.PodMonitor: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/podmonitors?resourceVersion=12829704\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.816566221Z stdout F level=warn ts=2024-03-15T18:37:34.816436367Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1alpha1.ScrapeConfig: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1alpha1/scrapeconfigs?resourceVersion=12829654\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.816645309Z stdout F level=error ts=2024-03-15T18:37:34.816552948Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1alpha1.ScrapeConfig: failed to list *v1alpha1.ScrapeConfig: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1alpha1/scrapeconfigs?resourceVersion=12829654\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.911075172Z stdout F level=warn ts=2024-03-15T18:37:34.910838351Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.Alertmanager: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/alertmanagers?resourceVersion=12829624\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.911122012Z stdout F level=error ts=2024-03-15T18:37:34.910949454Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.Alertmanager: failed to list *v1.Alertmanager: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/alertmanagers?resourceVersion=12829624\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.971395525Z stdout F level=warn ts=2024-03-15T18:37:34.971147299Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get \"https://10.96.0.1:443/api/v1/secrets?fieldSelector=type%21%3Dkubernetes.io%2Fdockercfg%2Ctype%21%3Dkubernetes.io%2Fservice-account-token%2Ctype%21%3Dhelm.sh%2Frelease.v1&resourceVersion=12829624\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.971432238Z stdout F level=error ts=2024-03-15T18:37:34.971251854Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get \"https://10.96.0.1:443/api/v1/secrets?fieldSelector=type%21%3Dkubernetes.io%2Fdockercfg%2Ctype%21%3Dkubernetes.io%2Fservice-account-token%2Ctype%21%3Dhelm.sh%2Frelease.v1&resourceVersion=12829624\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.992627602Z stdout F level=warn ts=2024-03-15T18:37:34.992424601Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get \"https://10.96.0.1:443/api/v1/secrets?fieldSelector=type%21%3Dkubernetes.io%2Fdockercfg%2Ctype%21%3Dkubernetes.io%2Fservice-account-token%2Ctype%21%3Dhelm.sh%2Frelease.v1&resourceVersion=12829690\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:34.99267895Z stdout F level=error ts=2024-03-15T18:37:34.99255784Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get \"https://10.96.0.1:443/api/v1/secrets?fieldSelector=type%21%3Dkubernetes.io%2Fdockercfg%2Ctype%21%3Dkubernetes.io%2Fservice-account-token%2Ctype%21%3Dhelm.sh%2Frelease.v1&resourceVersion=12829690\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.025989871Z stdout F level=warn ts=2024-03-15T18:37:35.025779143Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get \"https://10.96.0.1:443/api/v1/namespaces?resourceVersion=12829710\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.026017072Z stdout F level=error ts=2024-03-15T18:37:35.025900135Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://10.96.0.1:443/api/v1/namespaces?resourceVersion=12829710\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.069291276Z stdout F level=warn ts=2024-03-15T18:37:35.069116447Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get \"https://10.96.0.1:443/api/v1/configmaps?labelSelector=prometheus-name&resourceVersion=12829617\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.069379385Z stdout F level=error ts=2024-03-15T18:37:35.069238082Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get \"https://10.96.0.1:443/api/v1/configmaps?labelSelector=prometheus-name&resourceVersion=12829617\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.079512013Z stdout F level=warn ts=2024-03-15T18:37:35.079347063Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.PodMonitor: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/podmonitors?resourceVersion=12829658\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.079549746Z stdout F level=error ts=2024-03-15T18:37:35.079459787Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.PodMonitor: failed to list *v1.PodMonitor: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/podmonitors?resourceVersion=12829658\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.089900292Z stdout F level=warn ts=2024-03-15T18:37:35.089712176Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.StatefulSet: Get \"https://10.96.0.1:443/apis/apps/v1/statefulsets?resourceVersion=12829708\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.089937537Z stdout F level=error ts=2024-03-15T18:37:35.089831012Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://10.96.0.1:443/apis/apps/v1/statefulsets?resourceVersion=12829708\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.108204104Z stdout F level=warn ts=2024-03-15T18:37:35.108048862Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.PartialObjectMetadata: Get \"https://10.96.0.1:443/api/v1/secrets?fieldSelector=type%21%3Dkubernetes.io%2Fdockercfg%2Ctype%21%3Dkubernetes.io%2Fservice-account-token%2Ctype%21%3Dhelm.sh%2Frelease.v1&resourceVersion=12829690\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.108290713Z stdout F level=error ts=2024-03-15T18:37:35.108203276Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: Get \"https://10.96.0.1:443/api/v1/secrets?fieldSelector=type%21%3Dkubernetes.io%2Fdockercfg%2Ctype%21%3Dkubernetes.io%2Fservice-account-token%2Ctype%21%3Dhelm.sh%2Frelease.v1&resourceVersion=12829690\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.120450111Z stdout F level=warn ts=2024-03-15T18:37:35.120273605Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.ServiceMonitor: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/servicemonitors?resourceVersion=12829656\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.120483239Z stdout F level=error ts=2024-03-15T18:37:35.120395745Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.ServiceMonitor: failed to list *v1.ServiceMonitor: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/servicemonitors?resourceVersion=12829656\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.140715272Z stdout F level=warn ts=2024-03-15T18:37:35.140597772Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get \"https://10.96.0.1:443/api/v1/namespaces?resourceVersion=12829687\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.140745924Z stdout F level=error ts=2024-03-15T18:37:35.140714413Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://10.96.0.1:443/api/v1/namespaces?resourceVersion=12829687\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.162142044Z stdout F level=warn ts=2024-03-15T18:37:35.161963478Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1alpha1.AlertmanagerConfig: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1alpha1/alertmanagerconfigs?resourceVersion=12829615\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.162185271Z stdout F level=error ts=2024-03-15T18:37:35.1620938Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1alpha1.AlertmanagerConfig: failed to list *v1alpha1.AlertmanagerConfig: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1alpha1/alertmanagerconfigs?resourceVersion=12829615\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.185405096Z stdout F level=warn ts=2024-03-15T18:37:35.185245435Z caller=klog.go:108 component=k8s_client_runtime func=Warningf msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: failed to list *v1.PrometheusRule: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/prometheusrules?resourceVersion=12829719\": dial tcp 10.96.0.1:443: i/o timeout"
2024-03-15T18:37:35.185484401Z stdout F level=error ts=2024-03-15T18:37:35.185381291Z caller=klog.go:116 component=k8s_client_runtime func=ErrorDepth msg="pkg/mod/k8s.io/client-go@v0.28.4/tools/cache/reflector.go:229: Failed to watch *v1.PrometheusRule: failed to list *v1.PrometheusRule: Get \"https://10.96.0.1:443/apis/monitoring.coreos.com/v1/prometheusrules?resourceVersion=12829719\": dial tcp 10.96.0.1:443: i/o timeout"
2024-04-30T06:59:14.038789823Z stdout F level=info ts=2024-04-30T06:59:14.038429853Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2024-04-30T06:59:14.25843964Z stdout F level=info ts=2024-04-30T06:59:14.258259455Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2024-04-30T06:59:14.282096867Z stdout F level=info ts=2024-04-30T06:59:14.281866971Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2024-04-30T07:00:29.275295887Z stdout F level=info ts=2024-04-30T07:00:29.275099771Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2024-04-30T07:00:29.818680334Z stdout F level=info ts=2024-04-30T07:00:29.818431895Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2024-04-30T07:00:32.550813367Z stdout F level=info ts=2024-04-30T07:00:32.550608384Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2024-04-30T07:04:30.470810631Z stdout F level=info ts=2024-04-30T07:04:30.470542538Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2024-04-30T07:04:30.977176741Z stdout F level=info ts=2024-04-30T07:04:30.976933073Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2024-04-30T07:04:31.508137655Z stdout F level=info ts=2024-04-30T07:04:31.507953546Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2024-04-30T09:45:01.331000261Z stdout F level=info ts=2024-04-30T09:45:01.330665955Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2024-04-30T09:45:01.658294524Z stdout F level=info ts=2024-04-30T09:45:01.658099789Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2024-04-30T09:45:01.968428726Z stdout F level=info ts=2024-04-30T09:45:01.968190917Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2024-04-30T09:45:58.59308336Z stdout F level=info ts=2024-04-30T09:45:58.592860144Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
2024-04-30T09:45:58.814745352Z stdout F level=info ts=2024-04-30T09:45:58.814556688Z caller=operator.go:641 component=alertmanageroperator key=default/prometheus-kube-prometheus-alertmanager msg="sync alertmanager"
2024-04-30T09:45:58.842469316Z stdout F level=info ts=2024-04-30T09:45:58.842232419Z caller=operator.go:975 component=prometheusoperator key=default/prometheus-kube-prometheus-prometheus msg="sync prometheus"
